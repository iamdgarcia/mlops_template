name: MLOps Pipeline

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run drift detection daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  MLFLOW_TRACKING_URI: sqlite:///mlruns.db

jobs:
  security-scan:
    name: Security Vulnerability Scanning
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run Safety check for dependency vulnerabilities
      run: |
        safety check --json || echo "Safety check completed with warnings"
    
    - name: Run Bandit security linter
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ -ll -f screen
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
        retention-days: 30

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    needs: security-scan
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Check code formatting with black
      run: |
        black --check --diff src/ tests/ scripts/
    
    - name: Check import sorting with isort
      run: |
        isort --check-only --diff src/ tests/ scripts/
    
    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ tests/ scripts/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src/ tests/ scripts/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

  unit-tests:
    name: Unit and Integration Tests
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run unit tests with coverage
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  data-pipeline-validation:
    name: Data Pipeline Validation
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create necessary directories
      run: |
        mkdir -p data models logs mlruns
    
    - name: Run full data pipeline
      run: |
        python scripts/run_full_pipeline.py
    
    - name: Validate pipeline outputs
      run: |
        # Check that required data files were created
        test -f data/transactions_raw.csv || exit 1
        test -f data/transactions_processed.csv || exit 1
        test -f data/transactions_final.csv || exit 1
        echo "‚úì All required data files created"
    
    - name: Upload pipeline artifacts
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-outputs
        path: |
          data/*.csv
          data/*.json
          models/*.joblib
        retention-days: 7

  model-training:
    name: Model Training and Registration
    runs-on: ubuntu-latest
    needs: data-pipeline-validation
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v4
      with:
        name: pipeline-outputs
    
    - name: Create necessary directories
      run: |
        mkdir -p mlruns models logs
    
    - name: Train and register models
      run: |
        python -c '
        from src.pipelines import run_training_pipeline
        from src.config import ConfigManager
        
        config_manager = ConfigManager()
        config = config_manager.get_training_config()
        
        # Run training pipeline
        results = run_training_pipeline(config)
        print(f"Training complete. Best model: {results[\"best_model_name\"]}")
        print(f"Best ROC-AUC: {results[\"best_roc_auc\"]:.4f}")
        '
    
    - name: Upload model artifacts
      uses: actions/upload-artifact@v4
      with:
        name: trained-models
        path: |
          models/*.joblib
          mlruns/
        retention-days: 30

  docker-build:
    name: Build and Test Docker Image
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Build Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: false
        tags: fraud-detection:${{ github.sha }},fraud-detection:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Test Docker container
      run: |
        docker run --rm fraud-detection:latest python -c "import src; print('‚úì Container imports working')"

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [model-training, docker-build]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    environment:
      name: staging
      url: https://staging-fraud-detection.example.com
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download trained models
      uses: actions/download-artifact@v4
      with:
        name: trained-models
    
    - name: Deploy to staging environment
      run: |
        echo "üöÄ Deploying to staging environment"
        echo "‚úì Model artifacts downloaded"
        echo "‚úì Configuration validated"
        # Add your actual deployment commands here
        # Examples:
        # - kubectl apply -f k8s/staging/
        # - aws ecs update-service --cluster staging --service fraud-api
        # - gcloud run deploy fraud-api --region us-central1
    
    - name: Run health check
      run: |
        echo "üè• Running health checks on staging deployment"
        # Add health check commands
        # curl -f https://staging-fraud-detection.example.com/health || exit 1

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    environment:
      name: production
      url: https://fraud-detection.example.com
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download trained models
      uses: actions/download-artifact@v4
      with:
        name: trained-models
    
    - name: Deploy to production environment
      run: |
        echo "üöÄ Deploying to production environment"
        echo "‚úì Model artifacts downloaded"
        echo "‚úì Configuration validated"
        # Add your actual production deployment commands here
    
    - name: Run production health check
      run: |
        echo "üè• Running health checks on production deployment"
        # Add health check commands
    
    - name: Notify deployment
      run: |
        echo "üì¢ Production deployment complete"
        # Add notification logic (Slack, email, etc.)

  drift-detection:
    name: Drift Detection Monitoring
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run drift detection
      run: |
        echo "Running scheduled drift detection"
        python -c '
        from src.drift_detection import DriftDetector
        import pandas as pd
        import json
        from datetime import datetime
        
        # Load reference and current data
        reference_data = pd.read_csv("data/transactions_final.csv")
        # In production, load recent production data here
        current_data = reference_data.sample(n=1000, random_state=42)
        
        # Run drift detection
        with open("data/selected_features.json", "r") as f:
            feature_info = json.load(f)
            selected_features = feature_info.get("features", [])
        
        detector = DriftDetector(reference_data, selected_features)
        drift_results = {}
        
        for feature in selected_features[:5]:  # Check top 5 features
            result = detector.detect_feature_drift(current_data, feature)
            drift_results[feature] = result
            if result["drift_detected"]:
                print(f"WARNING: Drift detected in {feature}: p-value={result[\"p_value\"]:.4f}")
        
        print(f"Drift detection complete. Checked {len(drift_results)} features.")
        '
    
    - name: Create drift alert
      if: failure()
      run: |
        echo "Critical drift detected - creating alert"
        # Add alerting logic (PagerDuty, Slack, email, etc.)