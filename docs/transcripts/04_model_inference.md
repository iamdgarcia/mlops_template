# Module 4 — Model Inference (Recording Script)

Audience: ML newcomers learning consistent inference
Goal: Load the saved artefact and run single/batch predictions with the same feature logic as training

## Opening (00:00)
- Say: “We’ll reuse the same `FeatureEngineer` during inference through the `InferencePipeline` so training/serving parity holds.”

## Load Pipeline (00:30)
- Instantiate `InferencePipeline(model_path=serving_config['model']['local_model_path'])`.
- Print `pipeline.get_model_info()` → includes `model_type`, `feature_count` (from selected_features.json), and metadata.

## Single Prediction (01:00)
- Use `create_sample_transaction()` or a custom payload dict:
  - Required minimal fields:
    - `amount, merchant_category, transaction_type, location, device_type, hour_of_day, day_of_week`
  - Optional context:
    - `user_transaction_frequency, user_avg_amount, user_transaction_count`
- Run: `pipeline.predict_single(payload)` → keys include:
  - `fraud_prediction` (0/1), `fraud_probability` ∈ [0,1], `prediction_timestamp`

## Batch Prediction (02:00)
- Create a demo dataframe with ~1000 rows (or load your own CSV).
- Run: `predict_batch(df, include_probabilities=True)` → adds columns:
  - `fraud_prediction, fraud_probability, confidence_score, prediction_timestamp`
- Risk levels (notebook logic):
  - `risk_level = high if p ≥ threshold+0.2; medium if p ≥ threshold; else low`
  - threshold from `configs/serving_config.yaml: prediction.fraud_threshold`

## Export Results (03:00)
- Use the helper to save:
  - `data/inference_results/predictions_<ts>.csv`
  - `data/inference_results/fraud_cases_<ts>.csv`
  - `data/inference_results/high_risk_<ts>.csv`
  - `data/inference_results/inference_summary_<ts>.json` (includes risk distribution and file locations)

## Feature Parity Diagnostic (03:30)
- Show the diagnostic cell comparing:
  - Model’s expected features (`feature_names_in_` when available)
  - Features generated by `pipeline.preprocess_data(...)`
  - Display counts of missing/extra features for user trust

## Minimal Serving (Optional, 04:00)
- Say: “To simulate real‑time predictions, a tiny FastAPI server is included.”
- Run:
  - `uvicorn scripts.minimal_serve:app --reload --port 8000`
  - `curl -s localhost:8000/health | jq`
  - `curl -s -X POST localhost:8000/predict -H 'Content-Type: application/json' -d '{...}' | jq`
- Note: The server internally uses the same `InferencePipeline`.

## Close (04:30)
- “We have deterministic, parity‑safe predictions and exports. Next, monitoring and drift detection.”

*** End of Module 4 ***

